cache:
  paths:
    - maven.repository/
 
variables:
  MAVEN_OPTS: "-Dmaven.repo.local=maven.repository -Dorg.slf4j.simpleLogger.log.org.apache.maven.cli.transfer.Slf4jMavenTransferListener=WARN -Xmx4g -Dorg.lwjgl.util.Debug=true -Dorg.lwjgl.util.DebugLoader=true"
  JAVA_HOME: "/usr/lib/jvm/java-8-openjdk-amd64"
 
.base-job: &base-job
  before_script:
    # Installs Maven, Vulkan development libraries, etc.
    - apt-get update && apt-get install -y --no-install-recommends ca-certificates apt-transport-https gnupg-curl && \
      rm -rf /var/lib/apt/lists/* && \
      NVIDIA_GPGKEY_SUM=d1be581509378368edeec8c1eb2958702feedf3bc3d17011adbf24efacce4ab5 && \
      NVIDIA_GPGKEY_FPR=ae09fe4bbd223a84b2ccfce3f60f4b3d7fa2af80 && \
      apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/7fa2af80.pub && \
      apt-key adv --export --no-emit-version -a $NVIDIA_GPGKEY_FPR | tail -n +5 > cudasign.pub && \
      echo "$NVIDIA_GPGKEY_SUM  cudasign.pub" | sha256sum -c --strict - && rm cudasign.pub && \
      echo "deb https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64 /" > /etc/apt/sources.list.d/cuda.list && \
      echo "deb https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1604/x86_64 /" > /etc/apt/sources.list.d/nvidia-ml.list
    - apt-get update -qq --force-yes > /dev/null
    - apt-get install -qq --force-yes unzip wget git maven openjdk-8-jdk libvulkan1 libvulkan-dev vulkan-utils nvidia-opencl-icd nvidia-libopencl1 clinfo > /dev/null
    # Output Vulkan driver information, but do not fail in case of non-zero
    # return (happens e.g. if $DISPLAY is not set)
    - vulkaninfo || true
    - clinfo || true
    - wget -q https://ulrik.is/scenery-demo-models.zip
    - unzip -q scenery-demo-models.zip
  script:
    - mvn -B package
    - mvn -B -Pintegration-tests test -Dscenery.Renderer=VulkanRenderer -Dscenery.LogLevel=info 
    - tar cvjf results.tar.bz2 ExampleRunner-*
  artifacts:
    when: always
    expire_in: 48h
    paths:
      - results.tar.bz2

scenery-nvidia:
  image: nvidia/vulkan:1.1.121-cuda-10.1-alpha
  <<: *base-job
  after_script:
    - nvidia-smi
  tags:
    - cuda
    - intel

scenery-amd:
  image: rocm/rocm-terminal
  <<: *base-job
  variables:
    SUDO: "sudo"
    GPURUN: "sudo su -m - rocm-user -c"
  before_script:
    # The rocm docker container requires the user to be in the video group which
    # can usually be set via docker's --group-add option. GitLab-Runner currently
    # has no known option for doing that. Therefore, it manually has to happen in
    # the job description. 
    - $SUDO usermod -a -G video rocm-user
    # Installs Maven, Vulkan development libraries, etc.
    - $SUDO apt-get -qq --force-yes update > /dev/null
    - $SUDO apt-get install -qq --force-yes unzip kmod wget git maven openjdk-8-jdk libvulkan1 libvulkan-dev vulkan-utils > /dev/null
    # Installs the AMD GPUopen Vulkan driver
    - wget https://github.com/GPUOpen-Drivers/AMDVLK/releases/download/v-2019.Q3.6/amdvlk_2019.Q3.6_amd64.deb
    - $SUDO dpkg -i amdvlk_2019.Q3.6_amd64.deb
    - $SUDO apt-get -f install
    - $SUDO lsmod
    # Output Vulkan driver information, but do not fail in case of non-zero
    # return (happens e.g. if $DISPLAY is not set)
    - vulkaninfo || true
    - wget -q https://ulrik.is/scenery-demo-models.zip
    - unzip -q scenery-demo-models.zip
  after_script:
    - rocm-smi
  tags:
    - amd
    - rocm


